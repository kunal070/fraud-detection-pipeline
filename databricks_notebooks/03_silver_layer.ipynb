{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sl-md-0001",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "sl-md-0001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver Layer: Data Cleaning & Transformation\n",
    "\n",
    "## Purpose\n",
    "Read raw transaction data from the Bronze Delta table, apply cleaning and enrichment\n",
    "transformations, and write to the Silver Delta table.\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "Bronze Delta Table (Raw) -> Filter -> Enrich -> Deduplicate -> Silver Delta Table (Clean)\n",
    "```\n",
    "\n",
    "## Outputs\n",
    "- **Table:** `fraud_lakehouse_workspace.default.silver_transactions`\n",
    "- **Format:** Delta Lake\n",
    "- **New Columns:** transaction_date, transaction_hour, is_high_value, is_fraud, amount_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sl-md-0002",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "sl-md-0002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Cleanup (Optional)\n",
    "\n",
    "Run this cell to **reset the Silver table and checkpoint** before a fresh start.\n",
    "Skip this cell if you want to keep existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "sl-code-0001",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "sl-code-0001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Stop active streams\n",
    "for s in spark.streams.active:\n",
    "    s.stop()\n",
    "print(\"All active streams stopped.\")\n",
    "\n",
    "# 2. Drop the Silver table\n",
    "try:\n",
    "    spark.sql(\"DROP TABLE IF EXISTS fraud_lakehouse_workspace.default.silver_transactions\")\n",
    "    print(\"Silver table dropped.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to drop Silver table: {e}\")\n",
    "\n",
    "# 3. Clean the checkpoint directory\n",
    "silver_checkpoint = \"YOUR_CHECKPOINT_PATH_HERE\" # Replace with your actual checkpoint path, e.g., \"dbfs:/fraud_lakehouse_workspace/checkpoints/silver_transactions\"\n",
    "try:\n",
    "    dbutils.fs.rm(silver_checkpoint, recurse=True)\n",
    "    print(\"Checkpoint cleared.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to clear checkpoint: {e}\")\n",
    "\n",
    "print(\"Table and checkpoint deleted. Ready for a fresh start.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sl-md-0003",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "sl-md-0003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Silver Layer - Read, Transform & Write\n",
    "\n",
    "This cell performs the full Silver pipeline:\n",
    "- **Filter:** Remove rows with null Amount or Class\n",
    "- **Enrich:** Add transaction_date, transaction_hour, is_high_value, is_fraud, amount_category\n",
    "- **Deduplicate:** Remove duplicates based on eventhub_sequence and Time\n",
    "- **Select:** Pick only the columns needed downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "sl-code-0002",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "sl-code-0002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define checkpoint\n",
    "silver_checkpoint = \"YOUR_CHECKPOINT_PATH_HERE\" # Replace with your actual checkpoint path, e.g., \"dbfs:/fraud_lakehouse_workspace/checkpoints/silver_transactions\"\n",
    "\n",
    "print(\"Building Silver Layer...\")\n",
    "\n",
    "try:\n",
    "    # 1. Read from Bronze (streaming)\n",
    "    df_bronze_stream = spark.readStream \\\n",
    "        .option(\"ignoreDeletes\", \"true\") \\\n",
    "        .table(\"fraud_lakehouse_workspace.default.bronze_transactions\")\n",
    "\n",
    "    # 2. Silver Transformations\n",
    "    df_silver = df_bronze_stream \\\n",
    "        .filter(col(\"Amount\").isNotNull()) \\\n",
    "        .filter(col(\"Class\").isNotNull()) \\\n",
    "        .withColumn(\"transaction_date\", \n",
    "                    to_date(from_unixtime(col(\"Time\")))) \\\n",
    "        .withColumn(\"transaction_hour\", \n",
    "                    hour(from_unixtime(col(\"Time\")))) \\\n",
    "        .withColumn(\"is_high_value\", \n",
    "                    when(col(\"Amount\") > 1000, 1).otherwise(0)) \\\n",
    "        .withColumn(\"is_fraud\", \n",
    "                    when(col(\"Class\") == 1, 1).otherwise(0)) \\\n",
    "        .withColumn(\"amount_category\",\n",
    "                    when(col(\"Amount\") < 10, \"Small\")\n",
    "                    .when((col(\"Amount\") >= 10) & (col(\"Amount\") < 100), \"Medium\")\n",
    "                    .when((col(\"Amount\") >= 100) & (col(\"Amount\") < 1000), \"Large\")\n",
    "                    .otherwise(\"Very Large\")) \\\n",
    "        .withColumn(\"silver_processed_time\", current_timestamp()) \\\n",
    "        .dropDuplicates([\"eventhub_sequence\", \"Time\"]) \\\n",
    "        .select(\n",
    "            # Transaction details\n",
    "            col(\"Time\"),\n",
    "            col(\"transaction_date\"),\n",
    "            col(\"transaction_hour\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Class\"),\n",
    "            col(\"is_fraud\"),\n",
    "            col(\"is_high_value\"),\n",
    "            col(\"amount_category\"),\n",
    "            # PCA features (V1-V28)\n",
    "            *[col(f\"V{i}\") for i in range(1, 29)],\n",
    "            # Metadata\n",
    "            col(\"eventhub_enqueued_time\"),\n",
    "            col(\"eventhub_offset\"),\n",
    "            col(\"eventhub_sequence\"),\n",
    "            col(\"bronze_ingestion_time\"),\n",
    "            col(\"silver_processed_time\")\n",
    "        )\n",
    "\n",
    "    # 3. Write to Silver Delta Table\n",
    "    silver_query = df_silver.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", silver_checkpoint) \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .toTable(\"fraud_lakehouse_workspace.default.silver_transactions\")\n",
    "\n",
    "    print(\"Silver layer streaming!\")\n",
    "    print(f\"Stream ID: {silver_query.id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Silver layer setup failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sl-md-0004",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "sl-md-0004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Alternative: Simplified Write (Optional)\n",
    "\n",
    "A simplified version of the Silver write with minimal transformations.\n",
    "Use this if the full transformation above encounters issues or for a quick test.\n",
    "\n",
    "> **Note:** Only run this if the full pipeline above has not been started,\n",
    "> or after running the cleanup cell first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "sl-code-0003",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "sl-code-0003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, current_timestamp\n",
    "\n",
    "try:\n",
    "    # 1. Read Stream from Bronze\n",
    "    df_bronze_stream = spark.readStream \\\n",
    "        .option(\"ignoreDeletes\", \"true\") \\\n",
    "        .table(\"fraud_lakehouse_workspace.default.bronze_transactions\")\n",
    "\n",
    "    # 2. Transformation\n",
    "    df_silver = df_bronze_stream \\\n",
    "        .filter(col(\"Amount\").isNotNull()) \\\n",
    "        .withColumn(\"is_high_value\", when(col(\"Amount\") > 1000, 1).otherwise(0)) \\\n",
    "        .withColumn(\"silver_processed_time\", current_timestamp()) \\\n",
    "        .dropDuplicates([\"eventhub_sequence\"])\n",
    "\n",
    "    # 3. Write Stream (This will auto-create the table)\n",
    "    silver_query = df_silver.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", silver_checkpoint) \\\n",
    "        .toTable(\"fraud_lakehouse_workspace.default.silver_transactions\")\n",
    "\n",
    "    print(\"Simplified Silver table created and streaming!\")\n",
    "except Exception as e:\n",
    "    print(f\"Simplified Silver write failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sl-md-0005",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "sl-md-0005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Verify: Bronze vs Silver Record Counts\n",
    "\n",
    "Compare the total record counts in Bronze and Silver to confirm\n",
    "data is flowing correctly through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "sl-code-0004",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "sl-code-0004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    bronze_count = spark.table(\"fraud_lakehouse_workspace.default.bronze_transactions\").count()\n",
    "    silver_count = spark.table(\"fraud_lakehouse_workspace.default.silver_transactions\").count()\n",
    "\n",
    "    print(f\"Bronze: {bronze_count}\")\n",
    "    print(f\"Silver: {silver_count}\")\n",
    "\n",
    "    if bronze_count == silver_count:\n",
    "        print(\"PERFECT MATCH - counts are equal.\")\n",
    "    else:\n",
    "        print(\"Counts differ - Bronze may still be appending while Silver is running.\")\n",
    "except Exception as e:\n",
    "    print(f\"Verification failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sl-md-0006",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "sl-md-0006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "**Silver Layer Status:**\n",
    "- Read from Bronze Delta table (streaming)\n",
    "- Filtered null Amount and Class values\n",
    "- Enriched with transaction_date, transaction_hour, is_high_value, is_fraud, amount_category\n",
    "- Deduplicated on eventhub_sequence and Time\n",
    "- Written to Silver Delta table with schema evolution enabled\n",
    "\n",
    "**Next Step:** Run `04_gold_layer` to build analytics and aggregation tables."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_silver_layer",
   "notebookVersion": "0.0.1"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
