{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Setup: Azure Storage & Event Hubs Connection\n",
    "\n",
    "## Purpose\n",
    "This notebook establishes connections to:\n",
    "- **Azure Blob Storage** (for reading Stream Analytics output)\n",
    "- **Event Hubs** (for real-time streaming ingestion)\n",
    "\n",
    "## Prerequisites\n",
    "- Storage account access key\n",
    "- Event Hubs connection string\n",
    "\n",
    "## Outputs\n",
    "- Configured Spark session with storage access\n",
    "- Validated Event Hubs connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 1. Azure Blob Storage Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# Storage account credentials\n# NOTE: In production, use Azure Key Vault or Databricks Secrets instead of hardcoding\nstorage_account_name = \"YOUR_STORAGE_ACCOUNT_NAME\"  # Replace with your Azure Storage account name\ncontainer_name = \"eventhub-capture\"\nstorage_account_key = \"YOUR_KEY_HERE\"  # TODO: Replace with dbutils.secrets.get()\n\n# Configure Spark to access Azure Blob Storage\nspark.conf.set(\n    f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\",\n    storage_account_key\n)\n\nprint(f\"✅ Storage configured: wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\")"
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## 2. Verify Storage Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build storage path\n",
    "storage_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\"\n",
    "\n",
    "# Test connection by listing files\n",
    "try:\n",
    "    files = dbutils.fs.ls(storage_path)\n",
    "    print(f\"Connection successful! Found {len(files)} items in container.\")\n",
    "    display(dbutils.fs.ls(storage_path))\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "## 3. Event Hubs Configuration\n",
    "\n",
    "**Connection String Format:**\n",
    "```\n",
    "Endpoint=sb://NAMESPACE.servicebus.windows.net/;\n",
    "SharedAccessKeyName=KEY_NAME;\n",
    "SharedAccessKey=KEY;\n",
    "EntityPath=HUB_NAME\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event Hubs connection details\n",
    "# SECURITY NOTE: Use Databricks secrets in production\n",
    "CONNECTION_STRING = \"YOUR_CONNECTION_STRING_HERE\"  # TODO: Use dbutils.secrets\n",
    "\n",
    "# Encrypt connection string for Spark\n",
    "ehConf = {\n",
    "  'eventhubs.connectionString': sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(CONNECTION_STRING)\n",
    "}\n",
    "\n",
    "print(\"✅ Event Hubs configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "## 4. Validation Summary\n",
    "\n",
    "Run the cells above to verify:\n",
    "- ✅ Storage account accessible\n",
    "- ✅ Container files visible\n",
    "- ✅ Event Hubs connection string configured\n",
    "\n",
    "**Next Step:** Run `02_bronze_layer` to start ingesting data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}