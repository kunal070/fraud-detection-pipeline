{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa000001-0000-0000-0000-000000000001",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa000001-0000-0000-0000-000000000001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Layer: Raw Data Ingestion from Event Hubs\n",
    "\n",
    "## Purpose\n",
    "Stream raw transaction data from Azure Event Hubs and write to Delta Lake Bronze table.\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "Event Hubs ‚Üí Parse JSON ‚Üí Bronze Delta Table (Raw)\n",
    "```\n",
    "\n",
    "## Outputs\n",
    "- **Table:** `fraud_lakehouse_workspace.default.bronze_transactions`\n",
    "- **Format:** Delta Lake\n",
    "- **Schema:** All 30+ original columns + Event Hubs metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa000001-0000-0000-0000-000000000002",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa000001-0000-0000-0000-000000000002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "cc67280d-ab6d-43ad-a840-99749eb78bf1",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc67280d-ab6d-43ad-a840-99749eb78bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Event Hubs connection string WITH EntityPath\n",
    "CONNECTION_STRING = \"YOUR_KEY_HERE\" # Replace with your actual Event Hubs connection string\n",
    "\n",
    "# Encrypt the connection string\n",
    "ehConf = {\n",
    "  'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(CONNECTION_STRING)\n",
    "}\n",
    "\n",
    "print(\"Imports and configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa000001-0000-0000-0000-000000000003",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa000001-0000-0000-0000-000000000003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Connect to Event Hubs Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "b39348bc-8863-4eab-80f7-a4834dd34052",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b39348bc-8863-4eab-80f7-a4834dd34052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üì° Connecting to Event Hubs stream...\")\n",
    "\n",
    "# Read streaming data from Event Hubs\n",
    "try:\n",
    "    df_stream = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"eventhubs\") \\\n",
    "      .options(**ehConf) \\\n",
    "      .load()\n",
    "    print(\"Connected to Event Hubs stream\")\n",
    "    print(\"\\n Raw Event Hubs Schema:\")\n",
    "    df_stream.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa000001-0000-0000-0000-000000000004",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa000001-0000-0000-0000-000000000004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Define Fraud Transaction Schema\n",
    "\n",
    "The Event Hubs `body` column contains JSON with 30+ features:\n",
    "- **Time:** Seconds elapsed since first transaction\n",
    "- **V1-V28:** PCA-transformed features (anonymized)\n",
    "- **Amount:** Transaction amount in Euros\n",
    "- **Class:** 0 = Legitimate, 1 = Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "aa000002-0000-0000-0000-000000000001",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa000002-0000-0000-0000-000000000001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema for parsing JSON body\n",
    "fraud_schema = StructType([\n",
    "    StructField(\"Time\", DoubleType(), True),\n",
    "    StructField(\"V1\", DoubleType(), True),\n",
    "    StructField(\"V2\", DoubleType(), True),\n",
    "    StructField(\"V3\", DoubleType(), True),\n",
    "    StructField(\"V4\", DoubleType(), True),\n",
    "    StructField(\"V5\", DoubleType(), True),\n",
    "    StructField(\"V6\", DoubleType(), True),\n",
    "    StructField(\"V7\", DoubleType(), True),\n",
    "    StructField(\"V8\", DoubleType(), True),\n",
    "    StructField(\"V9\", DoubleType(), True),\n",
    "    StructField(\"V10\", DoubleType(), True),\n",
    "    StructField(\"V11\", DoubleType(), True),\n",
    "    StructField(\"V12\", DoubleType(), True),\n",
    "    StructField(\"V13\", DoubleType(), True),\n",
    "    StructField(\"V14\", DoubleType(), True),\n",
    "    StructField(\"V15\", DoubleType(), True),\n",
    "    StructField(\"V16\", DoubleType(), True),\n",
    "    StructField(\"V17\", DoubleType(), True),\n",
    "    StructField(\"V18\", DoubleType(), True),\n",
    "    StructField(\"V19\", DoubleType(), True),\n",
    "    StructField(\"V20\", DoubleType(), True),\n",
    "    StructField(\"V21\", DoubleType(), True),\n",
    "    StructField(\"V22\", DoubleType(), True),\n",
    "    StructField(\"V23\", DoubleType(), True),\n",
    "    StructField(\"V24\", DoubleType(), True),\n",
    "    StructField(\"V25\", DoubleType(), True),\n",
    "    StructField(\"V26\", DoubleType(), True),\n",
    "    StructField(\"V27\", DoubleType(), True),\n",
    "    StructField(\"V28\", DoubleType(), True),\n",
    "    StructField(\"Amount\", DoubleType(), True),\n",
    "    StructField(\"Class\", IntegerType(), True),\n",
    "    StructField(\"IngestionTime\", DoubleType(), True),\n",
    "    StructField(\"OriginalClass\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "print(\"Schema defined with 30+ columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa000001-0000-0000-0000-000000000005",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa000001-0000-0000-0000-000000000005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Parse JSON and Extract Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "aa000002-0000-0000-0000-000000000002",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa000002-0000-0000-0000-000000000002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parse JSON from Event Hubs body\n",
    "df_bronze = df_stream \\\n",
    "    .withColumn(\"body_string\", col(\"body\").cast(\"string\")) \\\n",
    "    .withColumn(\"transaction\", from_json(col(\"body_string\"), fraud_schema)) \\\n",
    "    .select(\n",
    "        # Transaction data (from JSON)\n",
    "        col(\"transaction.*\"),\n",
    "        # Event Hubs metadata\n",
    "        col(\"enqueuedTime\").alias(\"eventhub_enqueued_time\"),\n",
    "        col(\"offset\").alias(\"eventhub_offset\"),\n",
    "        col(\"sequenceNumber\").alias(\"eventhub_sequence\"),\n",
    "        col(\"partitionKey\").alias(\"eventhub_partition_key\"),\n",
    "        # Bronze ingestion timestamp\n",
    "        current_timestamp().alias(\"bronze_ingestion_time\")\n",
    "    )\n",
    "\n",
    "print(\"JSON parsed successfully\")\n",
    "print(\"\\n Bronze DataFrame Schema:\")\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa000001-0000-0000-0000-000000000006",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa000001-0000-0000-0000-000000000006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Write to Bronze Delta Table\n",
    "\n",
    "**Features:**\n",
    "- **Format:** Delta Lake (ACID transactions)\n",
    "- **Mode:** Append (streaming)\n",
    "- **Checkpoint:** Fault-tolerant recovery\n",
    "- **Schema Evolution:** Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "cc67280d-ab6d-43ad-a840-99749eb78bf2",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc67280d-ab6d-43ad-a840-99749eb78bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "checkpoint_path = \"YOUR_CHECKPOINT_PATH_HERE\" # Replace with your actual checkpoint path, e.g., \"dbfs:/fraud_lakehouse_workspace/checkpoints/bronze_transactions\"\n",
    "bronze_table_name = \"fraud_lakehouse_workspace.default.bronze_transactions\"\n",
    "\n",
    "print(\"Writing to Bronze Delta Lake...\")\n",
    "print(f\"Table: {bronze_table_name}\")\n",
    "print(f\"Checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# Clear old checkpoint if exists (optional - for fresh starts)\n",
    "try:\n",
    "    dbutils.fs.rm(checkpoint_path, recurse=True)\n",
    "    print(\"Old checkpoint cleared\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Start streaming write to Delta table\n",
    "bronze_query = df_bronze \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .toTable(bronze_table_name)\n",
    "\n",
    "print(\"\\nBronze streaming ACTIVE!\")\n",
    "print(f\"Stream ID: {bronze_query.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa000001-0000-0000-0000-000000000007",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa000001-0000-0000-0000-000000000007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Monitor Stream Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "aa000002-0000-0000-0000-000000000003",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa000002-0000-0000-0000-000000000003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Wait for initialization\n",
    "time.sleep(5)\n",
    "\n",
    "# Check stream status\n",
    "print(\"Stream Status:\")\n",
    "print(f\"  Active: {bronze_query.isActive}\")\n",
    "print(f\"  Status: {bronze_query.status}\")\n",
    "\n",
    "# Check progress\n",
    "if bronze_query.lastProgress:\n",
    "    print(f\"\\nLast Progress:\")\n",
    "    print(f\"  Input Rows: {bronze_query.lastProgress.get('numInputRows', 0)}\")\n",
    "    print(f\"  Processed: {bronze_query.lastProgress.get('processedRowsPerSecond', 0)}/sec\")\n",
    "else:\n",
    "    print(\"\\nWaiting for first batch...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa000001-0000-0000-0000-000000000008",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa000001-0000-0000-0000-000000000008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Verify Data in Bronze Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "aa000002-0000-0000-0000-000000000004",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa000002-0000-0000-0000-000000000004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from Bronze Delta table\n",
    "try:\n",
    "    bronze_df = spark.read.format(\"delta\").table(\"fraud_lakehouse_workspace.default.bronze_transactions\")\n",
    "\n",
    "    count = bronze_df.count()\n",
    "    print(f\"Bronze Table Records: {count}\")\n",
    "\n",
    "    if count > 0:\n",
    "        print(\"\\nSUCCESS! Data is flowing!\")\n",
    "        print(\"\\nSample transactions:\")\n",
    "        bronze_df.select(\"Time\", \"Amount\", \"Class\", \"bronze_ingestion_time\").show(10)\n",
    "\n",
    "        # Check fraud vs legit count\n",
    "        fraud_count = bronze_df.filter(col(\"Class\") == 1).count()\n",
    "        legit_count = bronze_df.filter(col(\"Class\") == 0).count()\n",
    "        print(f\"\\nFraud: {fraud_count} | Legit: {legit_count}\")\n",
    "\n",
    "        # Show latest enqueued time\n",
    "        latest_event = bronze_df.select(\"eventhub_enqueued_time\").orderBy(col(\"eventhub_enqueued_time\").desc()).first()[0]\n",
    "        print(f\"  Latest Event Enqueued: {latest_event}\")\n",
    "    else:\n",
    "        print(\"\\n‚è≥ No data yet - run simulator and wait ~30 seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"Verification failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa000001-0000-0000-0000-000000000009",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa000001-0000-0000-0000-000000000009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "**Bronze Layer Status:**\n",
    "- ‚úÖ Streaming from Event Hubs\n",
    "- ‚úÖ Writing to Delta Lake\n",
    "- ‚úÖ Checkpoint enabled (fault-tolerant)\n",
    "- ‚úÖ Schema evolution enabled\n",
    "\n",
    "**Next Step:** Run `04_silver_layer` to clean and transform data"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_bronze_layer",
   "notebookVersion": "0.0.1"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
